"""
í‰ê°€ ë©”íŠ¸ë¦­ ìœ í‹¸ë¦¬í‹° (Macro F1 Score ì‚¬ìš©)
"""
from sklearn.metrics import (
    f1_score, accuracy_score, precision_score, recall_score,
    classification_report, confusion_matrix
)
from typing import Dict, Tuple
import numpy as np
import torch


def calculate_metrics(
    y_true: np.ndarray,
    y_pred: np.ndarray,
    threshold: float = 0.5
) -> Dict:
    """
    ë¶„ë¥˜ ë©”íŠ¸ë¦­ ê³„ì‚° (Macro F1 Score ì‚¬ìš©)

    Args:
        y_true: ì‹¤ì œ ë¼ë²¨ (0=Real, 1=Fake)
        y_pred: ì˜ˆì¸¡ ë¼ë²¨ (ë˜ëŠ” í™•ë¥  ë°°ì—´ì¸ ê²½ìš° threshold ì ìš©)
        threshold: í™•ë¥  ë°°ì—´ì¸ ê²½ìš° ì‚¬ìš©í•  ì„ê³„ê°’

    Returns:
        ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    # í™•ë¥  ë°°ì—´ì¸ ê²½ìš° ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    if y_pred.dtype == float and (y_pred > 1.0).any() or (y_pred < 0.0).any():
        y_pred = (y_pred >= threshold).astype(int)
    else:
        y_pred = y_pred.astype(int)

    # Macro F1 Score (ëŒ€íšŒ í‰ê°€ ê¸°ì¤€)
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)

    # ì¶”ê°€ ë©”íŠ¸ë¦­
    accuracy = accuracy_score(y_true, y_pred)

    # í´ë˜ìŠ¤ë³„ F1 Score
    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)
    f1_real = f1_per_class[0] if len(f1_per_class) > 0 else 0.0
    f1_fake = f1_per_class[1] if len(f1_per_class) > 1 else 0.0

    # Precision, Recall
    precision_macro = precision_score(
        y_true, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(
        y_true, y_pred, average='macro', zero_division=0)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)

    # B. ë¼ë²¨ ë§¤í•‘ ê²€ì¦ ë¡œê·¸
    print(f"\n[B. ë¼ë²¨ ë§¤í•‘ ê²€ì¦]")
    print(f"  Real (0) - TP: {tn}, FP: {fn}")
    print(f"  Fake (1) - TP: {tp}, FP: {fp}")
    print(
        f"  Macro F1: {macro_f1:.4f} (Real: {f1_real:.4f}, Fake: {f1_fake:.4f})")

    return {
        'macro_f1': float(macro_f1),  # ëŒ€íšŒ í‰ê°€ ê¸°ì¤€
        'f1_score': float(macro_f1),  # í˜¸í™˜ì„±
        'f1_real': float(f1_real),
        'f1_fake': float(f1_fake),
        'accuracy': float(accuracy),
        'precision': float(precision_macro),
        'recall': float(recall_macro),
        'confusion_matrix': {
            'tn': int(tn),
            'fp': int(fp),
            'fn': int(fn),
            'tp': int(tp)
        }
    }


def find_optimal_threshold(
    y_true: np.ndarray,
    y_probs: np.ndarray,
    metric: str = 'macro_f1',
    threshold_range: Tuple[float, float] = (0.1, 0.9),
    step: float = 0.01
) -> Tuple[float, Dict]:
    """
    ìµœì  ë¶„ë¥˜ ì„ê³„ê°’ íƒìƒ‰ (D. ì„ê³„ê°’ íŠœë‹)

    Args:
        y_true: ì‹¤ì œ ë¼ë²¨ (0=Real, 1=Fake)
        y_probs: Fake í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ 
        metric: ìµœì í™”í•  ë©”íŠ¸ë¦­ ('macro_f1', 'f1_fake', 'f1_real')
        threshold_range: íƒìƒ‰í•  ì„ê³„ê°’ ë²”ìœ„
        step: íƒìƒ‰ ìŠ¤í…

    Returns:
        (ìµœì  ì„ê³„ê°’, í•´ë‹¹ ì„ê³„ê°’ì—ì„œì˜ ë©”íŠ¸ë¦­)
    """
    best_threshold = 0.5
    best_score = 0.0
    best_metrics = {}

    thresholds = np.arange(threshold_range[0], threshold_range[1] + step, step)

    for threshold in thresholds:
        y_pred = (y_probs >= threshold).astype(int)
        metrics = calculate_metrics(y_true, y_pred)

        if metric == 'macro_f1':
            score = metrics['macro_f1']
        elif metric == 'f1_fake':
            score = metrics['f1_fake']
        elif metric == 'f1_real':
            score = metrics['f1_real']
        else:
            score = metrics['macro_f1']

        if score > best_score:
            best_score = score
            best_threshold = threshold
            best_metrics = metrics

    return best_threshold, best_metrics


def print_classification_report(y_true: np.ndarray, y_pred: np.ndarray):
    """
    ë¶„ë¥˜ ë¦¬í¬íŠ¸ ì¶œë ¥

    Args:
        y_true: ì‹¤ì œ ë¼ë²¨
        y_pred: ì˜ˆì¸¡ ë¼ë²¨
    """
    print("\n" + "="*50)
    print("Classification Report")
    print("="*50)
    print(classification_report(y_true, y_pred, target_names=['Real', 'Fake']))
    print("="*50)

    # Macro F1 Score ê°•ì¡°
    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)
    print(f"\nğŸŒŸ Macro F1 Score (ëŒ€íšŒ í‰ê°€ ê¸°ì¤€): {macro_f1:.4f}")
    print("="*50 + "\n")
