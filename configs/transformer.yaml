# CNN 모델 설정 (Transformer 대신, 사전학습 모델 없이)

model:
  name: transformer # 호환성을 위해 유지, 실제로는 CNN 모델 사용
  num_classes: 2
  dropout: 0.3
  vocab_size: 30000 # 실제로는 학습 시 자동 설정됨
  embedding_dim: 300
  num_filters: 100
  filter_sizes: [3, 4, 5]

training:
  batch_size: 32
  learning_rate: 0.001
  num_epochs: 10
  val_ratio: 0.2
  random_state: 42

  optimizer: adam
  weight_decay: 0.0001

  # 스케줄러 설정
  scheduler: cosine # 'cosine', 'step', null

  # 조기 종료
  early_stopping:
    patience: 5
    min_delta: 0.001

data:
  data_dir: data
  max_length: 512

tokenizer:
  vocab_size: 30000
  max_length: 512
